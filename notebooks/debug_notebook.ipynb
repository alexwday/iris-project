{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:iris.src.chat_model.model_settings:Using environment: local\n",
      "INFO:iris.src.chat_model.model_settings:Using API base URL: https://api.openai.com/v1\n"
     ]
    }
   ],
   "source": [
    "# This enables auto-reloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import the model\n",
    "from iris.src.chat_model.model import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conversation for testing\n",
    "conversation = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in accounting.\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the definition of a business combination under IFRS, search internal wiki, perform 2 searches, just pick any documents\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# You can modify the conversation above to test different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:iris.src.chat_model.model_settings:Using small model: gpt-4o-mini-2024-07-18 in local environment\n",
      "INFO:iris.src.chat_model.model_settings:Using large model: gpt-4o-2024-08-06 in local environment\n",
      "INFO:iris.src.chat_model.model_settings:Using small model: gpt-4o-mini-2024-07-18 in local environment\n",
      "INFO:iris.src.chat_model.model_settings:Using small model: gpt-4o-mini-2024-07-18 in local environment\n",
      "INFO:iris.src.chat_model.model_settings:Using small model: gpt-4o-mini-2024-07-18 in local environment\n",
      "INFO:iris.src.chat_model.model_settings:Using large model: gpt-4o-2024-08-06 in local environment\n",
      "2025-04-04 12:32:07,906 - root - INFO - Logging system initialized\n",
      "2025-04-04 12:32:07,907 - root - INFO - Initializing model setup...\n",
      "2025-04-04 12:32:07,907 - iris.src.initial_setup.ssl.ssl - INFO - SSL certificate setup skipped in local environment\n",
      "2025-04-04 12:32:07,907 - iris.src.initial_setup.oauth.oauth - INFO - Using API key authentication from local settings\n",
      "2025-04-04 12:32:07,907 - iris.src.initial_setup.oauth.oauth - INFO - Using OpenAI API key from settings: sk-proj...\n",
      "2025-04-04 12:32:07,907 - root - INFO - Processing input conversation...\n",
      "2025-04-04 12:32:07,907 - iris.src.conversation_setup.conversation - INFO - Processed conversation: 2 messages filtered to 1 messages\n",
      "2025-04-04 12:32:07,907 - root - INFO - Conversation processed: 1 messages\n",
      "2025-04-04 12:32:07,907 - root - INFO - Getting routing decision...\n",
      "2025-04-04 12:32:07,907 - iris.src.agents.agent_router.router - INFO - Getting routing decision using model: gpt-4o-mini-2024-07-18\n",
      "2025-04-04 12:32:07,925 - iris.src.llm_connectors.rbc_openai - INFO - Using API key: sk-proj...\n",
      "2025-04-04 12:32:07,925 - iris.src.llm_connectors.rbc_openai - INFO - Using API base URL: https://api.openai.com/v1\n",
      "2025-04-04 12:32:07,925 - iris.src.llm_connectors.rbc_openai - INFO - Making non-streaming call to model: gpt-4o-mini-2024-07-18 with tools in local environment\n",
      "2025-04-04 12:32:07,925 - iris.src.llm_connectors.rbc_openai - INFO - Attempt 1/3: Sending request to OpenAI API\n",
      "2025-04-04 12:32:07,925 - iris.src.llm_connectors.rbc_openai - INFO - API call parameters (excluding message content): {'model': 'gpt-4o-mini-2024-07-18', 'max_tokens': 4096, 'temperature': 0.0, 'stream': False, 'timeout': 30}\n",
      "2025-04-04 12:32:08,692 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 12:32:08,700 - iris.src.llm_connectors.rbc_openai - INFO - Received response in 0.78 seconds\n",
      "2025-04-04 12:32:08,701 - iris.src.llm_connectors.rbc_openai - INFO - Token usage - Completion: 9 ($0.0000), Prompt: 2371 ($0.0000), Total: 2380 tokens, Total Cost: $0.0000\n",
      "2025-04-04 12:32:08,702 - iris.src.agents.agent_router.router - INFO - Routing decision: research_from_database\n",
      "2025-04-04 12:32:08,702 - root - INFO - Using research path based on routing decision\n",
      "2025-04-04 12:32:08,703 - root - INFO - Clarifying research needs...\n",
      "2025-04-04 12:32:08,703 - iris.src.agents.agent_clarifier.clarifier - INFO - Clarifying research needs using model: gpt-4o-mini-2024-07-18\n",
      "2025-04-04 12:32:08,715 - iris.src.llm_connectors.rbc_openai - INFO - Using API key: sk-proj...\n",
      "2025-04-04 12:32:08,715 - iris.src.llm_connectors.rbc_openai - INFO - Using API base URL: https://api.openai.com/v1\n",
      "2025-04-04 12:32:08,716 - iris.src.llm_connectors.rbc_openai - INFO - Making non-streaming call to model: gpt-4o-mini-2024-07-18 with tools in local environment\n",
      "2025-04-04 12:32:08,716 - iris.src.llm_connectors.rbc_openai - INFO - Attempt 1/3: Sending request to OpenAI API\n",
      "2025-04-04 12:32:08,716 - iris.src.llm_connectors.rbc_openai - INFO - API call parameters (excluding message content): {'model': 'gpt-4o-mini-2024-07-18', 'max_tokens': 4096, 'temperature': 0.0, 'stream': False, 'timeout': 30}\n",
      "2025-04-04 12:32:10,329 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 12:32:10,332 - iris.src.llm_connectors.rbc_openai - INFO - Received response in 1.62 seconds\n",
      "2025-04-04 12:32:10,333 - iris.src.llm_connectors.rbc_openai - INFO - Token usage - Completion: 59 ($0.0000), Prompt: 2918 ($0.0000), Total: 2977 tokens, Total Cost: $0.0000\n",
      "2025-04-04 12:32:10,334 - iris.src.agents.agent_clarifier.clarifier - INFO - Clarifier decision: create_research_statement\n",
      "2025-04-04 12:32:10,334 - iris.src.agents.agent_clarifier.clarifier - INFO - Is continuation: False\n",
      "2025-04-04 12:32:10,335 - root - INFO - Creating research statement, proceeding with research\n",
      "2025-04-04 12:32:10,336 - root - INFO - Creating database query plan...\n",
      "2025-04-04 12:32:10,336 - iris.src.agents.agent_planner.planner - INFO - Creating query plan using model: gpt-4o-mini-2024-07-18\n",
      "2025-04-04 12:32:10,337 - iris.src.agents.agent_planner.planner - INFO - Is continuation: False\n",
      "2025-04-04 12:32:10,347 - iris.src.llm_connectors.rbc_openai - INFO - Using API key: sk-proj...\n",
      "2025-04-04 12:32:10,348 - iris.src.llm_connectors.rbc_openai - INFO - Using API base URL: https://api.openai.com/v1\n",
      "2025-04-04 12:32:10,349 - iris.src.llm_connectors.rbc_openai - INFO - Making non-streaming call to model: gpt-4o-mini-2024-07-18 with tools in local environment\n",
      "2025-04-04 12:32:10,349 - iris.src.llm_connectors.rbc_openai - INFO - Attempt 1/3: Sending request to OpenAI API\n",
      "2025-04-04 12:32:10,350 - iris.src.llm_connectors.rbc_openai - INFO - API call parameters (excluding message content): {'model': 'gpt-4o-mini-2024-07-18', 'max_tokens': 4096, 'temperature': 0.0, 'stream': False, 'timeout': 30}\n",
      "2025-04-04 12:32:13,196 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 12:32:13,199 - iris.src.llm_connectors.rbc_openai - INFO - Received response in 2.85 seconds\n",
      "2025-04-04 12:32:13,200 - iris.src.llm_connectors.rbc_openai - INFO - Token usage - Completion: 106 ($0.0000), Prompt: 3376 ($0.0000), Total: 3482 tokens, Total Cost: $0.0000\n",
      "2025-04-04 12:32:13,201 - iris.src.agents.agent_planner.planner - INFO - Query plan created with 2 queries\n",
      "2025-04-04 12:32:13,201 - root - INFO - Query plan created with 2 queries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---\n",
      "# üìã Research Plan\n",
      "\n",
      "## Research Statement\n",
      "Search for the definition of a business combination under IFRS in the internal wiki. This will include looking for relevant entries that provide insights into the accounting treatment and criteria for business combinations as per IFRS standards.\n",
      "\n",
      "## Database Queries\n",
      "1. APG Wiki Entries: definition of business combination under IFRS\n",
      "2. IASB Standards and Interpretations: IFRS 3 Business Combinations definition and criteria\n",
      "---\n",
      "\n",
      "\n",
      "## üîç Query 1: APG Wiki Entries - definition of business combination under IFRS\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 12:32:13,203 - iris.src.agents.database_subagents.database_router - INFO - Routing query to database: internal_wiki\n",
      "2025-04-04 12:32:13,208 - iris.src.agents.database_subagents.internal_wiki.subagent - INFO - Querying Internal Wiki database: definition of business combination under IFRS\n",
      "2025-04-04 12:32:13,209 - iris.src.chat_model.model_settings - INFO - Using large model: gpt-4o-2024-08-06 in local environment\n",
      "2025-04-04 12:32:13,209 - iris.src.agents.database_subagents.internal_wiki.subagent - INFO - Fetching wiki catalog with query: definition of business combination under IFRS (environment: local)\n",
      "2025-04-04 12:32:13,226 - iris.src.agents.database_subagents.internal_wiki.subagent - INFO - Retrieved 3 catalog entries from database\n",
      "2025-04-04 12:32:13,226 - iris.src.agents.database_subagents.internal_wiki.subagent - INFO - Retrieved 3 catalog entries\n",
      "2025-04-04 12:32:13,227 - iris.src.agents.database_subagents.internal_wiki.subagent - INFO - Selecting relevant documents from catalog\n",
      "2025-04-04 12:32:13,227 - iris.src.chat_model.model_settings - INFO - Using small model: gpt-4o-mini-2024-07-18 in local environment\n",
      "2025-04-04 12:32:13,235 - iris.src.llm_connectors.rbc_openai - INFO - Using API key: sk-proj...\n",
      "2025-04-04 12:32:13,236 - iris.src.llm_connectors.rbc_openai - INFO - Using API base URL: https://api.openai.com/v1\n",
      "2025-04-04 12:32:13,236 - iris.src.llm_connectors.rbc_openai - INFO - Making non-streaming call to model: gpt-4o-mini-2024-07-18 in local environment\n",
      "2025-04-04 12:32:13,236 - iris.src.llm_connectors.rbc_openai - INFO - Attempt 1/3: Sending request to OpenAI API\n",
      "2025-04-04 12:32:13,237 - iris.src.llm_connectors.rbc_openai - INFO - API call parameters (excluding message content): {'model': 'gpt-4o-mini-2024-07-18', 'max_tokens': 200, 'temperature': 0.7, 'stream': False, 'timeout': 30}\n",
      "2025-04-04 12:32:13,809 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 12:32:13,812 - iris.src.llm_connectors.rbc_openai - INFO - Received response in 0.58 seconds\n",
      "2025-04-04 12:32:13,813 - iris.src.llm_connectors.rbc_openai - INFO - Token usage - Completion: 2 ($0.0000), Prompt: 368 ($0.0000), Total: 370 tokens, Total Cost: $0.0000 (Database: internal_wiki)\n",
      "2025-04-04 12:32:13,813 - iris.src.llm_connectors.rbc_openai - INFO - Updated token usage for database 'internal_wiki': 370 tokens\n",
      "2025-04-04 12:32:13,814 - iris.src.agents.database_subagents.internal_wiki.subagent - INFO - Selected document IDs: []\n",
      "2025-04-04 12:32:13,814 - iris.src.agents.database_subagents.internal_wiki.subagent - INFO - Selected 0 relevant documents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No relevant documents found in the Internal Wiki database. Please try refining your query or check other databases.\n",
      "\n",
      "---"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 12:32:13,815 - root - INFO - Completed database query 1/2: internal_wiki\n",
      "2025-04-04 12:32:13,815 - iris.src.agents.agent_judge.judge - INFO - Evaluating research progress using model: gpt-4o-mini-2024-07-18\n",
      "2025-04-04 12:32:13,816 - iris.src.agents.agent_judge.judge - INFO - Completed queries: 1, Remaining queries: 1\n",
      "2025-04-04 12:32:13,827 - iris.src.llm_connectors.rbc_openai - INFO - Using API key: sk-proj...\n",
      "2025-04-04 12:32:13,827 - iris.src.llm_connectors.rbc_openai - INFO - Using API base URL: https://api.openai.com/v1\n",
      "2025-04-04 12:32:13,828 - iris.src.llm_connectors.rbc_openai - INFO - Making non-streaming call to model: gpt-4o-mini-2024-07-18 with tools in local environment\n",
      "2025-04-04 12:32:13,828 - iris.src.llm_connectors.rbc_openai - INFO - Attempt 1/3: Sending request to OpenAI API\n",
      "2025-04-04 12:32:13,828 - iris.src.llm_connectors.rbc_openai - INFO - API call parameters (excluding message content): {'model': 'gpt-4o-mini-2024-07-18', 'max_tokens': 4096, 'temperature': 0.0, 'stream': False, 'timeout': 30}\n",
      "2025-04-04 12:32:15,529 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 12:32:15,532 - iris.src.llm_connectors.rbc_openai - INFO - Received response in 1.70 seconds\n",
      "2025-04-04 12:32:15,533 - iris.src.llm_connectors.rbc_openai - INFO - Token usage - Completion: 62 ($0.0000), Prompt: 3368 ($0.0000), Total: 3430 tokens, Total Cost: $0.0000\n",
      "2025-04-04 12:32:15,534 - iris.src.agents.agent_judge.judge - INFO - Research decision: continue_research\n",
      "2025-04-04 12:32:15,535 - iris.src.agents.agent_judge.judge - INFO - Reason: The initial query in the internal wiki did not yield specific results regarding the definition of a business combination under IFRS. The remaining query to the IASB database is essential to obtain authoritative information on the definition and criteria for business combinations as per IFRS standards....\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## üîç Query 2: IASB Standards and Interpretations - IFRS 3 Business Combinations definition and criteria\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 12:32:15,537 - iris.src.agents.database_subagents.database_router - INFO - Routing query to database: external_iasb\n",
      "2025-04-04 12:32:15,540 - iris.src.agents.database_subagents.external_iasb.subagent - INFO - Querying database: IFRS 3 Business Combinations definition and criteria\n",
      "2025-04-04 12:32:16,046 - iris.src.agents.database_subagents.external_iasb.subagent - INFO - Database query completed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    DATABASE RESULTS\n",
      "    \n",
      "    Query: IFRS 3 Business Combinations definition and criteria\n",
      "    \n",
      "    [This is a placeholder response.]\n",
      "    [In a production environment, this would return actual search results.]\n",
      "    \n",
      "    The following resources would be returned:\n",
      "    - Relevant documentation matching your query criteria\n",
      "    - Implementation guidance and examples\n",
      "    - Technical references and specifications\n",
      "    \n",
      "\n",
      "---"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 12:32:16,048 - root - INFO - Completed database query 2/2: external_iasb\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "## üìä Research Summary\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 12:32:16,049 - iris.src.agents.agent_summarizer.summarizer - INFO - Generating streaming research summary using model: gpt-4o-2024-08-06\n",
      "2025-04-04 12:32:16,050 - iris.src.agents.agent_summarizer.summarizer - INFO - Summarizing 2 completed queries\n",
      "2025-04-04 12:32:16,065 - iris.src.llm_connectors.rbc_openai - INFO - Using API key: sk-proj...\n",
      "2025-04-04 12:32:16,065 - iris.src.llm_connectors.rbc_openai - INFO - Using API base URL: https://api.openai.com/v1\n",
      "2025-04-04 12:32:16,066 - iris.src.llm_connectors.rbc_openai - INFO - Making streaming call to model: gpt-4o-2024-08-06 in local environment\n",
      "2025-04-04 12:32:16,066 - iris.src.llm_connectors.rbc_openai - INFO - Attempt 1/3: Sending request to OpenAI API\n",
      "2025-04-04 12:32:16,066 - iris.src.llm_connectors.rbc_openai - INFO - API call parameters (excluding message content): {'model': 'gpt-4o-2024-08-06', 'max_tokens': 4096, 'temperature': 0.1, 'stream': True, 'timeout': 30, 'stream_options': {'include_usage': True}}\n",
      "2025-04-04 12:32:16,575 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-04 12:32:16,576 - iris.src.llm_connectors.rbc_openai - INFO - Received response in 0.51 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "## Key Findings\n",
      "\n",
      "The research aimed to define a business combination under IFRS and explore the relevant accounting treatment and criteria as per IFRS standards. The process involved querying both internal and external databases to gather comprehensive insights.\n",
      "\n",
      "### Internal Wiki Insights\n",
      "\n",
      "- **Query 1** focused on the internal wiki to find the definition of a business combination under IFRS. However, the results were not explicitly detailed in the provided context, indicating a potential gap in the internal documentation or a lack of specific entries addressing the query directly. This suggests that while the internal wiki is a valuable resource for RBC-specific guidance, it may not always contain exhaustive definitions or criteria for IFRS standards.\n",
      "\n",
      "### External IASB Standards\n",
      "\n",
      "- **Query 2** targeted the external IASB database, which is the authoritative source for IFRS standards. Although the results were placeholders, the query was designed to retrieve the definition, criteria, and guidance related to IFRS 3, which governs business combinations. This query would typically yield comprehensive documentation, including the standard's text, implementation guidance, and technical references.\n",
      "\n",
      "### Considerations and Recommendations\n",
      "\n",
      "- **Internal vs. External Sources**: The internal wiki may not always provide detailed IFRS definitions, highlighting the importance of consulting external authoritative sources like the IASB for official standards and interpretations. This ensures that the most accurate and up-to-date information is used for accounting treatments.\n",
      "\n",
      "- **Gaps in Internal Documentation**: The absence of detailed results from the internal wiki suggests a need for enhancing internal documentation to include more comprehensive IFRS-related entries. This could improve the accessibility of critical information for RBC-specific applications.\n",
      "\n",
      "- **Importance of External Standards**: Given the placeholder nature of the IASB query results, it is crucial to access the actual IFRS 3 documentation for precise definitions and criteria. This standard outlines the principles for recognizing and measuring business combinations, including the identification of the acquirer, the determination of the acquisition date, and the recognition and measurement of the identifiable assets acquired, liabilities assumed, and any non-controlling interest.\n",
      "\n",
      "### Conclusion\n",
      "\n",
      "The research underscores the necessity of leveraging both internal and external resources to fully understand and apply IFRS standards, particularly for complex topics like business combinations. While internal resources provide RBC-specific guidance, external standards offer the authoritative framework required for compliance. Enhancing internal documentation to better align with external standards could bridge existing gaps and facilitate more efficient policy implementation. For detailed guidance on business combinations under IFRS, consulting the full text of IFRS 3 from the IASB is recommended."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 12:32:25,967 - iris.src.llm_connectors.rbc_openai - INFO - Token usage - Completion: 506 ($0.0000), Prompt: 2318 ($0.0000), Total: 2824 tokens, Total Cost: $0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---\n",
      "Completed 2 database queries.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-04 12:32:25,969 - root - INFO - Completed research process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---\n",
      "## Usage Statistics\n",
      "\n",
      "- Input tokens: 2318\n",
      "- Output tokens: 506\n",
      "- Total tokens: 2824\n",
      "- Cost: $0.000038\n",
      "- Time: 18.14 seconds\n",
      "\n",
      "### Database Token Usage\n",
      "\n",
      "**APG Wiki Entries**\n",
      "- Input tokens: 368\n",
      "- Output tokens: 2\n",
      "- Total tokens: 370\n",
      "- Cost: $0.000001\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Call the model function with our conversation and debug mode enabled\n",
    "result_generator = model(conversation, debug_mode=True)\n",
    "\n",
    "# Collect all response chunks and debug data\n",
    "response_chunks = []\n",
    "debug_data = None\n",
    "\n",
    "for chunk in result_generator:\n",
    "    # Check if this is debug data or a regular response chunk\n",
    "    if chunk.startswith(\"\\n\\nDEBUG_DATA:\"):\n",
    "        debug_data_json = chunk.replace(\"\\n\\nDEBUG_DATA:\", \"\")\n",
    "        import json\n",
    "        debug_data = json.loads(debug_data_json)\n",
    "    else:\n",
    "        response_chunks.append(chunk)\n",
    "        print(chunk, end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "outputs": [],
   "source": "# Set up a widescreen dashboard visualization for process analytics and token usage\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom datetime import datetime\nimport matplotlib.ticker as ticker\nfrom matplotlib.gridspec import GridSpec\nimport matplotlib.patches as patches\n\n# RBC color theme\nrbc_colors = {\n    'primary_blue': '#0051A5',  # RBC Royal Blue\n    'secondary_blue': '#0073AE', # RBC Secondary Blue\n    'accent_blue': '#00A7E1',   # RBC Light Blue\n    'dark_blue': '#003168',     # RBC Dark Blue\n    'gold': '#F4C63D',          # RBC Gold\n    'navy': '#14133B',          # RBC Navy\n    'red': '#C40022',           # RBC Red\n    'light_gray': '#EAEAEA',    # Light Gray for backgrounds\n    'medium_gray': '#D0D0D0'    # Medium Gray for tile backgrounds\n}\n\n# Set the matplotlib style\nplt.style.use('seaborn-v0_8-whitegrid')\n\nif debug_data:\n    # Sort the decisions by timestamp to ensure correct ordering\n    decisions_sorted = sorted(debug_data['decisions'], key=lambda x: datetime.fromisoformat(x['timestamp']))\n    \n    # Define stage order and display names\n    stage_order = [\n        'router',       # 1. Initial routing decision\n        'clarifier',    # 2. Clarify research needs\n        'planner',      # 3. Create query plan\n        'database_query', # 4. Execute database queries\n        'judge',        # 5. Evaluate research progress\n        'summary'       # 6. Generate final summary\n    ]\n    \n    # Better display names for stages\n    stage_display_names = {\n        'router': '1. Router',\n        'clarifier': '2. Clarifier',\n        'planner': '3. Planner',\n        'database_query': '4. Database Query',\n        'judge': '5. Judge',\n        'summary': '6. Summary'\n    }\n    \n    # Get the overall start and end times\n    overall_start = datetime.fromisoformat(debug_data['start_timestamp'])\n    overall_end = datetime.fromisoformat(debug_data['end_timestamp'])\n    total_duration = (overall_end - overall_start).total_seconds()\n    \n    # Create dataframe from stage data\n    stages_data = []\n    for decision in decisions_sorted:\n        stage = decision['stage']\n        timestamp = datetime.fromisoformat(decision['timestamp'])\n        if 'token_usage' in decision:\n            token_usage = decision['token_usage']\n            stages_data.append({\n                'stage': stage,\n                'display_name': stage_display_names.get(stage, stage),\n                'timestamp': timestamp,\n                'prompt_tokens': token_usage['prompt'],\n                'completion_tokens': token_usage['completion'],\n                'total_tokens': token_usage['total'],\n                'cost': token_usage['cost']\n            })\n    \n    # Create dataframe\n    df_stages = pd.DataFrame(stages_data)\n    \n    # Calculate relative time from start\n    df_stages['time_seconds'] = (df_stages['timestamp'] - overall_start).dt.total_seconds()\n    \n    # Group by stage and sum tokens\n    stage_totals = df_stages.groupby(['stage', 'display_name']).agg({\n        'prompt_tokens': 'sum',\n        'completion_tokens': 'sum',\n        'total_tokens': 'sum',\n        'cost': 'sum'\n    }).reset_index()\n    \n    # Sort by stage order\n    stage_totals['stage_order'] = stage_totals['stage'].map({s: i for i, s in enumerate(stage_order)})\n    stage_totals = stage_totals.sort_values('stage_order')\n    \n    # Create database data if available\n    db_data = []\n    if debug_data['tokens']['databases']:\n        for db_name, usage in debug_data['tokens']['databases'].items():\n            # Make display name more readable\n            display_name = db_name.replace('internal_', '').replace('external_', '').upper()\n            db_data.append({\n                'database': db_name,\n                'display_name': display_name,\n                'prompt_tokens': usage['prompt_tokens'],\n                'completion_tokens': usage['completion_tokens'],\n                'total_tokens': usage['total_tokens'],\n                'cost': usage['cost']\n            })\n        \n        # Create dataframe\n        df_db = pd.DataFrame(db_data)\n    \n    # Define colors for different stages\n    stage_colors = {\n        'router': rbc_colors['primary_blue'],\n        'clarifier': rbc_colors['secondary_blue'],\n        'planner': rbc_colors['accent_blue'],\n        'database_query': rbc_colors['navy'],\n        'judge': rbc_colors['gold'],\n        'summary': rbc_colors['red']\n    }\n    \n    # Create a clean dashboard layout\n    fig = plt.figure(figsize=(20, 16))\n    \n    # Create top stats tiles - fixed layout\n    # 1. Create a row of 5 tiles at the top - one for each stat and cost\n    stat_tiles = [\n        {\"title\": \"Total Time\", \"value\": f\"{total_duration:.2f} sec\", \"color\": rbc_colors['primary_blue']},\n        {\"title\": \"Total Tokens\", \"value\": f\"{debug_data['tokens']['total']:,}\", \"color\": rbc_colors['secondary_blue']},\n        {\"title\": \"Prompt Tokens\", \"value\": f\"{debug_data['tokens']['prompt']:,}\", \"color\": rbc_colors['accent_blue']},\n        {\"title\": \"Completion Tokens\", \"value\": f\"{debug_data['tokens']['completion']:,}\", \"color\": rbc_colors['gold']},\n        {\"title\": \"Total Cost\", \"value\": f\"${debug_data['cost']:.6f}\", \"color\": rbc_colors['red']}\n    ]\n    \n    # Create a simple 3-row grid layout (stats row, token charts row, timeline row)\n    # Create stats row - manually positioned rectangles \n    for i, stat in enumerate(stat_tiles):\n        # Calculate position for each tile\n        left = 0.05 + (i * 0.19)  # 5 tiles across the full width with small gaps\n        bottom = 0.85\n        width = 0.17\n        height = 0.10\n        \n        # Create a rectangle for the tile\n        rect = patches.Rectangle((left, bottom), width, height, \n                                linewidth=2, edgecolor=stat[\"color\"],\n                                facecolor=rbc_colors['medium_gray'], alpha=0.3)\n        fig.add_artist(rect)\n        \n        # Add text directly to the figure\n        fig.text(left + width/2, bottom + height*0.65, stat[\"title\"], \n                ha='center', va='center', fontsize=14, fontweight='bold', \n                color=rbc_colors['dark_blue'])\n        \n        fig.text(left + width/2, bottom + height*0.35, stat[\"value\"], \n                ha='center', va='center', fontsize=16, fontweight='bold',\n                color=stat[\"color\"])\n    \n    # Token Usage Charts Row - create two axes side by side\n    ax_tokens = fig.add_axes([0.05, 0.5, 0.42, 0.3])  # [left, bottom, width, height]\n    ax_db = fig.add_axes([0.53, 0.5, 0.42, 0.3])\n    \n    # Timeline row - larger chart at bottom\n    ax_timeline = fig.add_axes([0.05, 0.08, 0.9, 0.35])\n    \n    # Add title at the top\n    fig.text(0.5, 0.96, 'IRIS System Performance Dashboard', \n            ha='center', va='center', fontsize=24, fontweight='bold',\n            color=rbc_colors['dark_blue'])\n    \n    # 1. Create the Token Usage by Stage chart\n    bar_width = 0.35\n    x = np.arange(len(stage_totals))\n    \n    # Plot bar chart with larger bars\n    prompt_bars = ax_tokens.bar(x - bar_width/2, stage_totals['prompt_tokens'], bar_width, \n                         label='Prompt tokens', color=rbc_colors['primary_blue'])\n    completion_bars = ax_tokens.bar(x + bar_width/2, stage_totals['completion_tokens'], bar_width, \n                             label='Completion tokens', color=rbc_colors['gold'])\n    \n    # Add value labels on top of bars\n    for bars in [prompt_bars, completion_bars]:\n        for bar in bars:\n            height = bar.get_height()\n            ax_tokens.text(bar.get_x() + bar.get_width()/2., height + 50,\n                    f'{height:,}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n    \n    ax_tokens.set_xlabel('Processing Stage', fontsize=14, fontweight='bold')\n    ax_tokens.set_ylabel('Number of Tokens', fontsize=14, fontweight='bold')\n    ax_tokens.set_title('Token Usage by Processing Stage', fontsize=18, fontweight='bold', color=rbc_colors['dark_blue'])\n    ax_tokens.set_xticks(x)\n    ax_tokens.set_xticklabels(stage_totals['display_name'], rotation=45, ha='right', fontsize=12)\n    ax_tokens.tick_params(axis='y', labelsize=12)\n    \n    # Format y-axis with comma for thousands\n    ax_tokens.yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n    \n    # Add legend with better position\n    ax_tokens.legend(loc='upper right', frameon=True, framealpha=0.9, fontsize=12)\n    \n    # Set background color\n    ax_tokens.set_facecolor(rbc_colors['light_gray'])\n    \n    # Add extra space at top for labels\n    y_top = max([max(stage_totals['prompt_tokens']), max(stage_totals['completion_tokens'])]) * 1.15\n    ax_tokens.set_ylim(0, y_top)\n    \n    # 2. Create the Database Usage chart\n    if len(db_data) > 0:\n        # Create a grouped bar chart\n        x = np.arange(len(df_db))\n        \n        # Plot bar chart with larger bars\n        prompt_bars = ax_db.bar(x - bar_width/2, df_db['prompt_tokens'], bar_width, \n                             label='Prompt tokens', color=rbc_colors['primary_blue'])\n        completion_bars = ax_db.bar(x + bar_width/2, df_db['completion_tokens'], bar_width, \n                                 label='Completion tokens', color=rbc_colors['gold'])\n        \n        # Add value labels on top of bars\n        for bars in [prompt_bars, completion_bars]:\n            for bar in bars:\n                height = bar.get_height()\n                ax_db.text(bar.get_x() + bar.get_width()/2., height + 5,\n                        f'{height:,}', ha='center', va='bottom', fontweight='bold', fontsize=10)\n        \n        ax_db.set_xlabel('Database', fontsize=14, fontweight='bold')\n        ax_db.set_ylabel('Number of Tokens', fontsize=14, fontweight='bold')\n        ax_db.set_title('Token Usage by Database', fontsize=18, fontweight='bold', color=rbc_colors['dark_blue'])\n        ax_db.set_xticks(x)\n        ax_db.set_xticklabels(df_db['display_name'], rotation=45, ha='right', fontsize=12)\n        ax_db.tick_params(axis='y', labelsize=12)\n        \n        # Format y-axis with comma for thousands\n        ax_db.yaxis.set_major_formatter(ticker.StrMethodFormatter('{x:,.0f}'))\n        \n        # Add legend in a non-overlapping position\n        ax_db.legend(loc='upper right', frameon=True, framealpha=0.9, fontsize=12)\n        \n        # Set background color\n        ax_db.set_facecolor(rbc_colors['light_gray'])\n        \n        # Add extra space at top for labels\n        y_top = max([max(df_db['prompt_tokens']), max(df_db['completion_tokens'])]) * 1.15\n        ax_db.set_ylim(0, y_top)\n    \n    # 3. Create the Detailed Timeline Visualization\n    # We need to keep ALL stage transitions in chronological order\n    detailed_timeline = []\n    \n    for i, decision in enumerate(decisions_sorted):\n        stage = decision['stage']\n        timestamp = datetime.fromisoformat(decision['timestamp'])\n        \n        # End time is either the next decision's timestamp or the end timestamp\n        if i + 1 < len(decisions_sorted):\n            end_time = datetime.fromisoformat(decisions_sorted[i+1]['timestamp'])\n        else:\n            end_time = overall_end\n        \n        # Calculate relative times\n        start_seconds = (timestamp - overall_start).total_seconds()\n        end_seconds = (end_time - overall_start).total_seconds()\n        duration = end_seconds - start_seconds\n        \n        # Add to detailed timeline\n        detailed_timeline.append({\n            'stage': stage,\n            'display_name': stage_display_names.get(stage, stage),\n            'start_seconds': start_seconds,\n            'end_seconds': end_seconds,\n            'duration': duration\n        })\n    \n    # Create dataframe from detailed timeline\n    df_detailed = pd.DataFrame(detailed_timeline)\n    \n    # Sort by start time to ensure chronological order\n    df_detailed = df_detailed.sort_values('start_seconds')\n    \n    # Increase spacing between bars - MUCH larger height for visibility\n    height = 0.8\n    spacing = 1.5\n    \n    # Create a map of stage to positions (for consistent vertical position)\n    unique_stages = list(stage_display_names.keys())\n    stage_positions = {stage: i * spacing for i, stage in enumerate(reversed(unique_stages))}\n    \n    # Plot horizontal bars for each transition\n    for i, row in df_detailed.iterrows():\n        stage = row['stage']\n        duration = row['duration']\n        start_pos = row['start_seconds']\n        duration_width = duration\n        color = stage_colors.get(stage, 'gray')\n        \n        # Use the stage's consistent position\n        position = stage_positions[stage]\n        \n        # Create the bar with proper position and width\n        ax_timeline.barh(position, duration_width, left=start_pos, height=height, color=color)\n        \n        # Add text label for bars with sufficient width\n        if duration_width > 1.5:\n            text_pos = start_pos + duration_width/2\n            label = f\"{row['duration']:.2f}s\"\n            ax_timeline.text(text_pos, position, label, ha='center', va='center',\n                    color='white', fontweight='bold', fontsize=12)\n    \n    # Adjust y-ticks and labels\n    positions = list(stage_positions.values())\n    stage_names = [stage_display_names[s] for s in reversed(unique_stages)]\n    \n    ax_timeline.set_yticks(positions)\n    ax_timeline.set_yticklabels(stage_names, fontsize=12)\n    \n    ax_timeline.set_xlabel('Time (seconds)', fontsize=14, fontweight='bold')\n    ax_timeline.set_title('Timeline of Processing Stages', fontsize=18, fontweight='bold', color=rbc_colors['dark_blue'])\n    ax_timeline.tick_params(axis='x', labelsize=12)\n    \n    # Add gridlines and format x-axis with decimal places\n    ax_timeline.grid(axis='x', linestyle='--', alpha=0.7)\n    ax_timeline.xaxis.set_major_formatter(ticker.StrMethodFormatter('{x:.2f}s'))\n    \n    # Set background color\n    ax_timeline.set_facecolor(rbc_colors['light_gray'])\n    \n    # Set appropriate x and y limits\n    ax_timeline.set_xlim(0, total_duration * 1.02)  # Add a small margin\n    ax_timeline.set_ylim(-1, max(positions) + 1)\n    \n    # Add legend for timeline stages \n    handles = [patches.Patch(color=color, label=stage_display_names.get(stage, stage)) \n              for stage, color in stage_colors.items()]\n    ax_timeline.legend(handles=handles, loc='upper right', fontsize=12, frameon=True, framealpha=0.7)\n    \n    # Adjust figure layout\n    plt.show()\nelse:\n    print(\"No debug data available. Make sure to run with debug_mode=True.\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CAPM Document Token Size Analysis\n",
    "\n",
    "This notebook analyzes the token sizes of various components of the CAPM documents in the database.\n",
    "We'll examine:\n",
    "1. Catalog descriptions\n",
    "2. Section summaries\n",
    "3. Section content\n",
    "\n",
    "The results will help us understand the token consumption in the CAPM subagent pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required modules\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "import tiktoken\n",
    "\n",
    "# Add the project root to the path if needed\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.append(project_root)\n",
    "\n",
    "# Import directly from the module\n",
    "import iris.src.initial_setup.db_config as db_config\n",
    "from iris.src.chat_model.model_settings import ENVIRONMENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# OpenAI tokenizer (using cl100k_base which is used by GPT-4 and Claude)\n",
    "def count_tokens(string: str) -> int:\n",
    "    \"\"\"Count the number of tokens in a string using tiktoken.\"\"\"\n",
    "    if not string:\n",
    "        return 0\n",
    "    encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "    return len(encoding.encode(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Function to connect to the database\n",
    "def connect_to_db(env=None):\n",
    "    \"\"\"Connect to the database in the specified environment.\"\"\"\n",
    "    if env is None:\n",
    "        # Use the default environment from model_settings\n",
    "        env = ENVIRONMENT\n",
    "    \n",
    "    print(f\"Connecting to database in '{env}' environment...\")\n",
    "    conn = db_config.connect_to_db(env)\n",
    "    if conn:\n",
    "        print(\"Connected successfully! ðŸŽ‰\\n\")\n",
    "        return conn\n",
    "    else:\n",
    "        print(\"Failed to connect to database.\")\n",
    "        return None\n",
    "\n",
    "# Connect to default environment\n",
    "conn = connect_to_db()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Analyze CAPM Catalog Descriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_capm_catalog_descriptions(conn):\n",
    "    \"\"\"Analyze token size of CAPM document descriptions in the catalog.\"\"\"\n",
    "    if not conn:\n",
    "        return\n",
    "    \n",
    "    print(\"Analyzing CAPM catalog descriptions...\")\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT id, document_name, document_description\n",
    "            FROM apg_catalog\n",
    "            WHERE document_source = 'internal_capm'\n",
    "            ORDER BY document_name\n",
    "        \"\"\")\n",
    "        \n",
    "        records = cur.fetchall()\n",
    "        if not records:\n",
    "            print(\"No CAPM records found in catalog.\")\n",
    "            return\n",
    "        \n",
    "        results = []\n",
    "        total_tokens = 0\n",
    "        \n",
    "        for doc_id, name, desc in records:\n",
    "            tokens = count_tokens(desc or \"\")\n",
    "            total_tokens += tokens\n",
    "            results.append({\n",
    "                \"document_id\": doc_id,\n",
    "                \"document_name\": name,\n",
    "                \"description_tokens\": tokens,\n",
    "                \"description_first_50_chars\": (desc or \"\")[:50] + (\"...\" if desc and len(desc) > 50 else \"\")\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"Total documents: {len(records)}\")\n",
    "        print(f\"Total tokens in all descriptions: {total_tokens}\")\n",
    "        print(f\"Average tokens per description: {total_tokens / len(records):.2f}\")\n",
    "        print(f\"Max tokens in a description: {df['description_tokens'].max()}\")\n",
    "        print(f\"Min tokens in a description: {df['description_tokens'].min()}\")\n",
    "        print(f\"Median tokens in a description: {df['description_tokens'].median()}\\n\")\n",
    "        \n",
    "        # Show distribution\n",
    "        token_ranges = [\n",
    "            (0, 50), (51, 100), (101, 200), \n",
    "            (201, 300), (301, 400), (401, 500), \n",
    "            (501, 1000), (1001, float('inf'))\n",
    "        ]\n",
    "        \n",
    "        print(\"Distribution of description token sizes:\")\n",
    "        for low, high in token_ranges:\n",
    "            count = ((df['description_tokens'] >= low) & (df['description_tokens'] <= high)).sum()\n",
    "            if high == float('inf'):\n",
    "                print(f\"  {low}+ tokens: {count} documents ({count/len(records)*100:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  {low}-{high} tokens: {count} documents ({count/len(records)*100:.1f}%)\")\n",
    "        \n",
    "        # Return the DataFrame for further analysis\n",
    "        return df\n",
    "\n",
    "catalog_df = analyze_capm_catalog_descriptions(conn)\n",
    "display(catalog_df.sort_values(by='description_tokens', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Analyze CAPM Section Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_capm_section_summaries(conn):\n",
    "    \"\"\"Analyze token size of CAPM section summaries.\"\"\"\n",
    "    if not conn:\n",
    "        return\n",
    "    \n",
    "    print(\"Analyzing CAPM section summaries...\")\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT c.document_name, c.section_id, c.section_name, c.section_summary\n",
    "            FROM apg_content c\n",
    "            JOIN apg_catalog cat ON c.document_name = cat.document_name AND c.document_source = cat.document_source\n",
    "            WHERE c.document_source = 'internal_capm'\n",
    "            ORDER BY c.document_name, c.section_id\n",
    "        \"\"\")\n",
    "        \n",
    "        records = cur.fetchall()\n",
    "        if not records:\n",
    "            print(\"No CAPM section summaries found.\")\n",
    "            return\n",
    "        \n",
    "        results = []\n",
    "        total_tokens = 0\n",
    "        total_by_doc = {}\n",
    "        \n",
    "        for doc_name, section_id, section_name, summary in records:\n",
    "            tokens = count_tokens(summary or \"\")\n",
    "            total_tokens += tokens\n",
    "            \n",
    "            # Track totals by document\n",
    "            if doc_name not in total_by_doc:\n",
    "                total_by_doc[doc_name] = 0\n",
    "            total_by_doc[doc_name] += tokens\n",
    "            \n",
    "            results.append({\n",
    "                \"document_name\": doc_name,\n",
    "                \"section_id\": section_id,\n",
    "                \"section_name\": section_name,\n",
    "                \"summary_tokens\": tokens,\n",
    "                \"summary_first_50_chars\": (summary or \"\")[:50] + (\"...\" if summary and len(summary) > 50 else \"\")\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"Total sections: {len(records)}\")\n",
    "        print(f\"Total tokens in all summaries: {total_tokens}\")\n",
    "        print(f\"Average tokens per summary: {total_tokens / len(records):.2f}\")\n",
    "        print(f\"Max tokens in a summary: {df['summary_tokens'].max()}\")\n",
    "        print(f\"Min tokens in a summary: {df['summary_tokens'].min()}\")\n",
    "        print(f\"Median tokens in a summary: {df['summary_tokens'].median()}\\n\")\n",
    "        \n",
    "        # Document-level statistics\n",
    "        doc_stats = pd.DataFrame([\n",
    "            {\"document_name\": doc, \"total_summary_tokens\": tokens}\n",
    "            for doc, tokens in total_by_doc.items()\n",
    "        ])\n",
    "        \n",
    "        print(f\"Number of unique documents: {len(doc_stats)}\")\n",
    "        print(f\"Average tokens in summaries per document: {doc_stats['total_summary_tokens'].mean():.2f}\")\n",
    "        print(f\"Max tokens in summaries per document: {doc_stats['total_summary_tokens'].max()}\")\n",
    "        print(f\"Min tokens in summaries per document: {doc_stats['total_summary_tokens'].min()}\")\n",
    "        print(f\"Median tokens in summaries per document: {doc_stats['total_summary_tokens'].median()}\\n\")\n",
    "        \n",
    "        # Return the DataFrame for further analysis\n",
    "        return df, doc_stats\n",
    "\n",
    "summary_df, doc_summary_stats = analyze_capm_section_summaries(conn)\n",
    "display(summary_df.sort_values(by='summary_tokens', ascending=False).head(10))\n",
    "print(\"\\nDocuments with highest total summary tokens:\")\n",
    "display(doc_summary_stats.sort_values(by='total_summary_tokens', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Analyze CAPM Section Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_capm_section_content(conn):\n",
    "    \"\"\"Analyze token size of CAPM section content.\"\"\"\n",
    "    if not conn:\n",
    "        return\n",
    "    \n",
    "    print(\"Analyzing CAPM section content...\")\n",
    "    with conn.cursor() as cur:\n",
    "        cur.execute(\"\"\"\n",
    "            SELECT c.document_name, c.section_id, c.section_name, \n",
    "                   length(c.section_content) as content_length\n",
    "            FROM apg_content c\n",
    "            JOIN apg_catalog cat ON c.document_name = cat.document_name AND c.document_source = cat.document_source\n",
    "            WHERE c.document_source = 'internal_capm'\n",
    "            ORDER BY c.document_name, c.section_id\n",
    "        \"\"\")\n",
    "        \n",
    "        records = cur.fetchall()\n",
    "        if not records:\n",
    "            print(\"No CAPM section content found.\")\n",
    "            return\n",
    "        \n",
    "        results = []\n",
    "        total_char_len = 0\n",
    "        total_by_doc = {}\n",
    "        \n",
    "        for doc_name, section_id, section_name, char_len in records:\n",
    "            # Estimate tokens from character length\n",
    "            tokens = int(char_len * 0.25)  # approximate conversion\n",
    "            total_char_len += char_len\n",
    "            \n",
    "            # Track totals by document\n",
    "            if doc_name not in total_by_doc:\n",
    "                total_by_doc[doc_name] = {\"char_len\": 0, \"num_sections\": 0}\n",
    "            total_by_doc[doc_name][\"char_len\"] += char_len\n",
    "            total_by_doc[doc_name][\"num_sections\"] += 1\n",
    "            \n",
    "            results.append({\n",
    "                \"document_name\": doc_name,\n",
    "                \"section_id\": section_id,\n",
    "                \"section_name\": section_name,\n",
    "                \"content_char_length\": char_len,\n",
    "                \"content_estimated_tokens\": tokens\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(results)\n",
    "        total_tokens = int(total_char_len * 0.25)\n",
    "        \n",
    "        # Sample a few sections to get exact token counts for verification\n",
    "        print(\"Sampling a few sections for exact token counts...\")\n",
    "        sampled_sections = 0\n",
    "        total_chars_sampled = 0\n",
    "        total_tokens_sampled = 0\n",
    "        \n",
    "        cur.execute(\"\"\"\n",
    "            SELECT document_name, section_id, section_content\n",
    "            FROM apg_content\n",
    "            WHERE document_source = 'internal_capm'\n",
    "            ORDER BY random()\n",
    "            LIMIT 10\n",
    "        \"\"\")\n",
    "        \n",
    "        for doc_name, section_id, content in cur.fetchall():\n",
    "            if content:\n",
    "                sampled_sections += 1\n",
    "                chars = len(content)\n",
    "                tokens = count_tokens(content)\n",
    "                total_chars_sampled += chars\n",
    "                total_tokens_sampled += tokens\n",
    "                print(f\"  Section {doc_name}/{section_id}: {chars} chars, {tokens} tokens (ratio: {tokens/chars:.4f})\")\n",
    "        \n",
    "        if sampled_sections > 0:\n",
    "            avg_ratio = total_tokens_sampled / total_chars_sampled\n",
    "            print(f\"\\nAverage tokens/char ratio from sample: {avg_ratio:.4f}\")\n",
    "            adjusted_total_tokens = int(total_char_len * avg_ratio)\n",
    "            print(f\"Adjusted total token estimate: {adjusted_total_tokens} (vs. {total_tokens} using 0.25 ratio)\\n\")\n",
    "            # Update the 0.25 estimate with the sampled ratio\n",
    "            tokens_per_char = avg_ratio\n",
    "            total_tokens = adjusted_total_tokens\n",
    "        else:\n",
    "            tokens_per_char = 0.25\n",
    "        \n",
    "        # Summary statistics\n",
    "        print(f\"Total sections: {len(records)}\")\n",
    "        print(f\"Total characters in all content: {total_char_len:,}\")\n",
    "        print(f\"Estimated total tokens in all content: {total_tokens:,}\")\n",
    "        print(f\"Average characters per section: {total_char_len / len(records):,.2f}\")\n",
    "        print(f\"Average estimated tokens per section: {total_tokens / len(records):,.2f}\")\n",
    "        print(f\"Max characters in a section: {df['content_char_length'].max():,}\")\n",
    "        print(f\"Min characters in a section: {df['content_char_length'].min():,}\")\n",
    "        print(f\"Median characters in a section: {df['content_char_length'].median():,.2f}\\n\")\n",
    "        \n",
    "        # Document-level statistics\n",
    "        doc_stats = pd.DataFrame([\n",
    "            {\n",
    "                \"document_name\": doc, \n",
    "                \"num_sections\": stats[\"num_sections\"],\n",
    "                \"total_char_length\": stats[\"char_len\"],\n",
    "                \"total_estimated_tokens\": int(stats[\"char_len\"] * tokens_per_char)\n",
    "            }\n",
    "            for doc, stats in total_by_doc.items()\n",
    "        ])\n",
    "        \n",
    "        print(f\"Number of unique documents: {len(doc_stats)}\")\n",
    "        print(f\"Average characters per document: {doc_stats['total_char_length'].mean():,.2f}\")\n",
    "        print(f\"Average estimated tokens per document: {doc_stats['total_estimated_tokens'].mean():,.2f}\")\n",
    "        print(f\"Max characters in a document: {doc_stats['total_char_length'].max():,}\")\n",
    "        print(f\"Min characters in a document: {doc_stats['total_char_length'].min():,}\")\n",
    "        print(f\"Median characters in a document: {doc_stats['total_char_length'].median():,.2f}\\n\")\n",
    "        \n",
    "        # Token size distribution\n",
    "        token_ranges = [\n",
    "            (0, 1000), (1001, 2000), (2001, 5000), \n",
    "            (5001, 10000), (10001, 20000), (20001, 50000), \n",
    "            (50001, float('inf'))\n",
    "        ]\n",
    "        \n",
    "        print(\"Distribution of document token sizes:\")\n",
    "        for low, high in token_ranges:\n",
    "            count = ((doc_stats['total_estimated_tokens'] >= low) & (doc_stats['total_estimated_tokens'] <= high)).sum()\n",
    "            if high == float('inf'):\n",
    "                print(f\"  {low:,}+ tokens: {count} documents ({count/len(doc_stats)*100:.1f}%)\")\n",
    "            else:\n",
    "                print(f\"  {low:,}-{high:,} tokens: {count} documents ({count/len(doc_stats)*100:.1f}%)\")\n",
    "        \n",
    "        # Return the DataFrames for further analysis\n",
    "        return df, doc_stats\n",
    "\n",
    "content_df, doc_content_stats = analyze_capm_section_content(conn)\n",
    "display(content_df.sort_values(by='content_estimated_tokens', ascending=False).head(10))\n",
    "print(\"\\nDocuments with highest total content tokens:\")\n",
    "display(doc_content_stats.sort_values(by='total_estimated_tokens', ascending=False).head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyze Overall CAPM Token Usage in Query Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def summarize_token_usage(catalog_df, summary_df, content_df, doc_summary_stats, doc_content_stats):\n",
    "    \"\"\"Summarize token usage in the CAPM subagent pipeline.\"\"\"\n",
    "    print(\"\\n=== CAPM Subagent Pipeline Token Usage ===\\n\")\n",
    "    \n",
    "    # Document Catalog Phase\n",
    "    total_catalog_tokens = catalog_df['description_tokens'].sum()\n",
    "    print(f\"1. Document Catalog Phase\")\n",
    "    print(f\"   Total tokens in all catalog descriptions: {total_catalog_tokens:,}\")\n",
    "    print(f\"   Average tokens per document description: {catalog_df['description_tokens'].mean():.2f}\")\n",
    "    print(f\"   This is the total token count sent to the LLM for document selection.\\n\")\n",
    "    \n",
    "    # Section Summary Phase\n",
    "    avg_per_doc = doc_summary_stats['total_summary_tokens'].mean()\n",
    "    median_per_doc = doc_summary_stats['total_summary_tokens'].median()\n",
    "    max_per_doc = doc_summary_stats['total_summary_tokens'].max()\n",
    "    \n",
    "    print(f\"2. Section Summary Phase\")\n",
    "    print(f\"   Average tokens in section summaries per document: {avg_per_doc:.2f}\")\n",
    "    print(f\"   Median tokens in section summaries per document: {median_per_doc:.2f}\")\n",
    "    print(f\"   Maximum tokens in section summaries for a single document: {max_per_doc}\")\n",
    "    print(f\"   This is the typical token count sent to the LLM for section selection within a document.\\n\")\n",
    "    \n",
    "    # Document Content Phase\n",
    "    avg_content_per_doc = doc_content_stats['total_estimated_tokens'].mean()\n",
    "    median_content_per_doc = doc_content_stats['total_estimated_tokens'].median()\n",
    "    max_content_per_doc = doc_content_stats['total_estimated_tokens'].max()\n",
    "    \n",
    "    print(f\"3. Document Content Phase\")\n",
    "    print(f\"   Average tokens in content per document: {avg_content_per_doc:,.2f}\")\n",
    "    print(f\"   Median tokens in content per document: {median_content_per_doc:,.2f}\")\n",
    "    print(f\"   Maximum tokens in content for a single document: {max_content_per_doc:,}\")\n",
    "    print(f\"   Based on document size distribution:\")\n",
    "    \n",
    "    # Calculate number of documents that would exceed token limits\n",
    "    token_limits = [8000, 16000, 32000, 64000, 128000]\n",
    "    for limit in token_limits:\n",
    "        exceeds = (doc_content_stats['total_estimated_tokens'] > limit).sum()\n",
    "        percent = exceeds / len(doc_content_stats) * 100\n",
    "        print(f\"   - {exceeds} documents ({percent:.1f}%) exceed {limit:,} token limit\")\n",
    "    \n",
    "    # Realistic scenario with multi-document synthesis\n",
    "    print(\"\\n4. Realistic Multi-Document Synthesis Scenarios\")\n",
    "    \n",
    "    # Sort documents by token size and create cumulative scenarios\n",
    "    sorted_docs = doc_content_stats.sort_values('total_estimated_tokens', ascending=False).reset_index()\n",
    "    \n",
    "    # Typical scenarios (top N documents)\n",
    "    for n_docs in [1, 2, 3, 5, 10]:\n",
    "        if n_docs <= len(sorted_docs):\n",
    "            total_tokens = sorted_docs.iloc[:n_docs]['total_estimated_tokens'].sum()\n",
    "            print(f\"   Top {n_docs} largest documents: {total_tokens:,} tokens total\")\n",
    "    \n",
    "    # Middle-sized documents\n",
    "    if len(sorted_docs) >= 10:\n",
    "        middle_start = max(0, len(sorted_docs)//2 - 5)\n",
    "        middle_tokens = sorted_docs.iloc[middle_start:middle_start+10]['total_estimated_tokens'].sum()\n",
    "        print(f\"   10 medium-sized documents (from middle): {middle_tokens:,} tokens total\")\n",
    "    \n",
    "    # Bottom-sized documents\n",
    "    if len(sorted_docs) >= 10:\n",
    "        bottom_tokens = sorted_docs.iloc[-10:]['total_estimated_tokens'].sum()\n",
    "        print(f\"   10 smallest documents: {bottom_tokens:,} tokens total\")\n",
    "\n",
    "if 'catalog_df' in locals() and 'summary_df' in locals() and 'content_df' in locals():\n",
    "    summarize_token_usage(catalog_df, summary_df, content_df, doc_summary_stats, doc_content_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Close the database connection\n",
    "if conn:\n",
    "    conn.close()\n",
    "    print(\"Database connection closed.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IRIS System Evaluation Notebook\n",
    "\n",
    "This notebook provides tools for evaluating the IRIS system against expected outputs and tracking agent decisions at each stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This enables auto-reloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import time\n",
    "from IPython.display import display, HTML\n",
    "from iris.src.chat_model.model import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Case Definition\n",
    "\n",
    "First, let's define a structure for our test cases. Each test case should include:\n",
    "- A unique ID\n",
    "- A description of the test case\n",
    "- The input conversation\n",
    "- Expected outputs at various stages (e.g., expected route, expected clarification, expected databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TC001</td>\n",
       "      <td>Simple IFRS 16 lease query</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TC002</td>\n",
       "      <td>Revenue recognition with complete context</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TC003</td>\n",
       "      <td>Internal controls query</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                description\n",
       "0  TC001                 Simple IFRS 16 lease query\n",
       "1  TC002  Revenue recognition with complete context\n",
       "2  TC003                    Internal controls query"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define some sample test cases\n",
    "test_cases = [\n",
    "    {\n",
    "        \"id\": \"TC001\",\n",
    "        \"description\": \"Simple IFRS 16 lease query\",\n",
    "        \"conversation\": {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful accounting assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"How do I account for lease modifications under IFRS 16?\"}\n",
    "            ]\n",
    "        },\n",
    "        \"expected\": {\n",
    "            \"router\": \"research_from_database\",\n",
    "            \"clarifier\": \"request_essential_context\",\n",
    "            \"requested_context\": [\"type of modification\"],\n",
    "            \"databases\": [\"external_iasb\", \"external_kpmg\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC002\",\n",
    "        \"description\": \"Revenue recognition with complete context\",\n",
    "        \"conversation\": {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful accounting assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"What is the process for recognizing revenue for software licenses under IFRS 15?\"}\n",
    "            ]\n",
    "        },\n",
    "        \"expected\": {\n",
    "            \"router\": \"research_from_database\",\n",
    "            \"clarifier\": \"request_essential_context\",\n",
    "            \"requested_context\": [\"type of software license\", \"additional services\"],\n",
    "            \"databases\": [\"external_iasb\", \"internal_wiki\"]\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"TC003\",\n",
    "        \"description\": \"Internal controls query\",\n",
    "        \"conversation\": {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are a helpful accounting assistant.\"},\n",
    "                {\"role\": \"user\", \"content\": \"What are the key internal controls for accounts payable?\"}\n",
    "            ]\n",
    "        },\n",
    "        \"expected\": {\n",
    "            \"router\": \"research_from_database\",\n",
    "            \"clarifier\": \"request_essential_context\",\n",
    "            \"requested_context\": [\"industry\", \"regulations\"],\n",
    "            \"databases\": [\"internal_icfr\", \"internal_par\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# Convert to DataFrame for easier management\n",
    "test_df = pd.DataFrame(test_cases)\n",
    "test_df[['id', 'description']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Tests and Collecting Results\n",
    "\n",
    "Now, let's create a function to run each test case and collect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_case(test_case):\n",
    "    \"\"\"Run a single test case and collect results from various stages.\"\"\"\n",
    "    \n",
    "    print(f\"Running test case {test_case['id']}: {test_case['description']}\")\n",
    "    \n",
    "    results = {\n",
    "        \"id\": test_case[\"id\"],\n",
    "        \"description\": test_case[\"description\"],\n",
    "        \"start_time\": time.time(),\n",
    "        \"stages\": [],\n",
    "        \"final_response\": \"\",\n",
    "        \"response_type\": \"\"\n",
    "    }\n",
    "    \n",
    "    # Run the model\n",
    "    try:\n",
    "        result_generator = model(test_case[\"conversation\"])\n",
    "        \n",
    "        # Get the initial chunk with metadata\n",
    "        initial_chunk = next(result_generator)\n",
    "        results[\"response_type\"] = initial_chunk.get(\"type\", \"None\")\n",
    "        \n",
    "        # Capture the response\n",
    "        response_text = \"\"\n",
    "        for chunk in result_generator:\n",
    "            if \"stage\" in chunk:\n",
    "                results[\"stages\"].append({\n",
    "                    \"name\": chunk.get(\"stage\", \"unknown\"),\n",
    "                    \"details\": chunk\n",
    "                })\n",
    "            \n",
    "            if chunk.get(\"response_chunk\"):\n",
    "                response_text += chunk[\"response_chunk\"]\n",
    "        \n",
    "        results[\"final_response\"] = response_text\n",
    "        \n",
    "    except Exception as e:\n",
    "        results[\"error\"] = str(e)\n",
    "    \n",
    "    results[\"end_time\"] = time.time()\n",
    "    results[\"duration\"] = results[\"end_time\"] - results[\"start_time\"]\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run a test case to see the output structure\n",
    "# sample_result = run_test_case(test_cases[0])\n",
    "# print(json.dumps(sample_result, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running Multiple Tests\n",
    "\n",
    "Now let's run all our test cases and collect the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_all_tests(test_cases):\n",
    "    \"\"\"Run all test cases and collect results.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for test_case in test_cases:\n",
    "        result = run_test_case(test_case)\n",
    "        results.append(result)\n",
    "        print(f\"Completed test case {test_case['id']} in {result['duration']:.2f} seconds\")\n",
    "        print(f\"Response type: {result['response_type']}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Uncomment to run all tests\n",
    "# all_results = run_all_tests(test_cases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing Single Test Case with Detailed Logging\n",
    "\n",
    "Let's create a function to run a single test case with detailed logging of each stage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_test_case(test_case_id):\n",
    "    \"\"\"Run a single test case with detailed logging and analysis.\"\"\"\n",
    "    \n",
    "    # Find the test case\n",
    "    test_case = next((tc for tc in test_cases if tc[\"id\"] == test_case_id), None)\n",
    "    if not test_case:\n",
    "        print(f\"Test case {test_case_id} not found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Analyzing test case {test_case['id']}: {test_case['description']}\")\n",
    "    print(\"Input conversation:\")\n",
    "    for i, msg in enumerate(test_case[\"conversation\"][\"messages\"]):\n",
    "        print(f\"  Message {i+1}: {msg['role']} - {msg['content'][:100]}{'...' if len(msg['content']) > 100 else ''}\")\n",
    "    print(\"\\nExpected outcomes:\")\n",
    "    for key, value in test_case[\"expected\"].items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\nRunning test...\")\n",
    "    result = run_test_case(test_case)\n",
    "    \n",
    "    print(f\"\\nResponse type: {result['response_type']}\")\n",
    "    print(f\"Execution time: {result['duration']:.2f} seconds\")\n",
    "    \n",
    "    if \"error\" in result:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\nStages:\")\n",
    "    for stage in result.get(\"stages\", []):\n",
    "        print(f\"  Stage: {stage['name']}\")\n",
    "        for k, v in stage[\"details\"].items():\n",
    "            if k != \"stage\":\n",
    "                print(f\"    {k}: {v}\")\n",
    "    \n",
    "    print(\"\\nFinal response:\")\n",
    "    print(result[\"final_response\"])\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Example usage\n",
    "# detailed_result = analyze_test_case(\"TC001\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Expected vs. Actual Analysis\n",
    "\n",
    "Let's create a function to compare expected vs. actual results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(test_case_id, result):\n",
    "    \"\"\"Compare expected vs. actual results for a test case.\"\"\"\n",
    "    \n",
    "    # Find the test case\n",
    "    test_case = next((tc for tc in test_cases if tc[\"id\"] == test_case_id), None)\n",
    "    if not test_case:\n",
    "        print(f\"Test case {test_case_id} not found\")\n",
    "        return\n",
    "    \n",
    "    expected = test_case[\"expected\"]\n",
    "    \n",
    "    # Extract actual values from the result\n",
    "    actual = {\n",
    "        \"router\": None,\n",
    "        \"clarifier\": None,\n",
    "        \"requested_context\": [],\n",
    "        \"databases\": []\n",
    "    }\n",
    "    \n",
    "    # Extract actual values from stages\n",
    "    for stage in result.get(\"stages\", []):\n",
    "        if stage[\"name\"] == \"router_decision\":\n",
    "            actual[\"router\"] = stage[\"details\"].get(\"decision\")\n",
    "        elif stage[\"name\"] == \"clarifier_decision\":\n",
    "            actual[\"clarifier\"] = stage[\"details\"].get(\"decision\")\n",
    "            if \"context_questions\" in stage[\"details\"]:\n",
    "                actual[\"requested_context\"] = stage[\"details\"][\"context_questions\"]\n",
    "        elif stage[\"name\"] == \"query_plan\":\n",
    "            if \"queries\" in stage[\"details\"]:\n",
    "                actual[\"databases\"] = [q.get(\"database\") for q in stage[\"details\"][\"queries\"]]\n",
    "    \n",
    "    # Compare expected vs. actual\n",
    "    comparison = {}\n",
    "    for key in expected.keys():\n",
    "        expected_value = expected.get(key)\n",
    "        actual_value = actual.get(key)\n",
    "        \n",
    "        if key in [\"requested_context\", \"databases\"]:\n",
    "            # For lists, check if any expected items appear in actual\n",
    "            matches = []\n",
    "            for exp_item in expected_value:\n",
    "                found = False\n",
    "                for act_item in actual_value:\n",
    "                    if isinstance(act_item, str) and exp_item.lower() in act_item.lower():\n",
    "                        found = True\n",
    "                        break\n",
    "                matches.append(found)\n",
    "            match_percentage = sum(matches) / len(matches) if matches else 0\n",
    "            comparison[key] = {\n",
    "                \"expected\": expected_value,\n",
    "                \"actual\": actual_value,\n",
    "                \"match\": match_percentage == 1,\n",
    "                \"match_percentage\": match_percentage\n",
    "            }\n",
    "        else:\n",
    "            # For simple values, check for exact match\n",
    "            match = False\n",
    "            if actual_value and expected_value:\n",
    "                match = expected_value == actual_value\n",
    "            comparison[key] = {\n",
    "                \"expected\": expected_value,\n",
    "                \"actual\": actual_value,\n",
    "                \"match\": match\n",
    "            }\n",
    "    \n",
    "    # Calculate overall match percentage\n",
    "    matches = [c[\"match\"] for c in comparison.values()]\n",
    "    match_percentage = sum(matches) / len(matches) if matches else 0\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\nTest case {test_case_id} comparison:\")\n",
    "    print(f\"Overall match: {match_percentage:.0%}\")\n",
    "    \n",
    "    for key, comp in comparison.items():\n",
    "        print(f\"\\n{key}:\")\n",
    "        print(f\"  Expected: {comp['expected']}\")\n",
    "        print(f\"  Actual: {comp['actual']}\")\n",
    "        if \"match_percentage\" in comp:\n",
    "            print(f\"  Match: {comp['match_percentage']:.0%}\")\n",
    "        else:\n",
    "            print(f\"  Match: {'✓' if comp['match'] else '✗'}\")\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Example usage\n",
    "# result = analyze_test_case(\"TC001\")\n",
    "# comparison = compare_results(\"TC001\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Tests and Analyze\n",
    "\n",
    "Now you can run any test case and analyze its results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment to run a specific test case\n",
    "# result = analyze_test_case(\"TC001\")\n",
    "# comparison = compare_results(\"TC001\", result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding Your Own Test Cases\n",
    "\n",
    "You can add your own test cases to the list by following this pattern:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_test_case(test_id, description, conversation, expected_outcome):\n",
    "    \"\"\"Add a new test case to the test_cases list.\"\"\"\n",
    "    new_test = {\n",
    "        \"id\": test_id,\n",
    "        \"description\": description,\n",
    "        \"conversation\": conversation,\n",
    "        \"expected\": expected_outcome\n",
    "    }\n",
    "    \n",
    "    # Check if test ID already exists\n",
    "    existing_ids = [tc[\"id\"] for tc in test_cases]\n",
    "    if test_id in existing_ids:\n",
    "        print(f\"Warning: Test case {test_id} already exists. It will be overwritten.\")\n",
    "        test_cases[:] = [tc for tc in test_cases if tc[\"id\"] != test_id]\n",
    "    \n",
    "    test_cases.append(new_test)\n",
    "    print(f\"Added test case {test_id}: {description}\")\n",
    "    \n",
    "    # Update the DataFrame\n",
    "    global test_df\n",
    "    test_df = pd.DataFrame(test_cases)\n",
    "    \n",
    "    return new_test\n",
    "\n",
    "# Example usage\n",
    "# new_test = add_test_case(\n",
    "#     \"TC004\",\n",
    "#     \"IFRS vs GAAP comparison\",\n",
    "#     {\n",
    "#         \"messages\": [\n",
    "#             {\"role\": \"system\", \"content\": \"You are a helpful accounting assistant.\"},\n",
    "#             {\"role\": \"user\", \"content\": \"What are the key differences between IFRS and US GAAP for revenue recognition?\"}\n",
    "#         ]\n",
    "#     },\n",
    "#     {\n",
    "#         \"router\": \"research_from_database\",\n",
    "#         \"clarifier\": \"request_essential_context\",\n",
    "#         \"requested_context\": [\"specific industry\", \"time period\"],\n",
    "#         \"databases\": [\"external_iasb\", \"external_ey\", \"external_kpmg\"]\n",
    "#     }\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

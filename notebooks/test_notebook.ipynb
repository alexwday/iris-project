{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1169096d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This enables auto-reloading of modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Import the model\n",
    "from iris.src.chat_model.model import model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8e42f72-04cf-4a2c-88cf-c774934d1167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define conversation for testing\n",
    "conversation = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful assistant specialized in accounting.\"},\n",
    "        {\"role\": \"user\", \"content\": \"Hello, can you help me find information about revenue recognition?\"},\n",
    "        {\"role\": \"assistant\", \"content\": \"I'd be happy to help with revenue recognition. What specific information are you looking to understand?\"},\n",
    "        {\"role\": \"user\", \"content\": \"I need to understand how to recognize revenue for long-term contracts.\"}\n",
    "    ]\n",
    "}\n",
    "\n",
    "# You can modify the conversation above to test different scenarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fa7cccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:iris.src.chat_model.model_settings:Using small model: gpt-4o-mini-2024-07-18 in local environment\n",
      "INFO:iris.src.chat_model.model_settings:Using large model: gpt-4o-2024-08-06 in local environment\n",
      "INFO:iris.src.chat_model.model_settings:Using small model: gpt-4o-mini-2024-07-18 in local environment\n",
      "INFO:iris.src.chat_model.model_settings:Using small model: gpt-4o-mini-2024-07-18 in local environment\n",
      "INFO:iris.src.chat_model.model_settings:Using small model: gpt-4o-mini-2024-07-18 in local environment\n",
      "2025-04-01 14:40:49,083 - root - INFO - Logging system initialized\n",
      "2025-04-01 14:40:49,083 - root - INFO - Initializing model setup...\n",
      "2025-04-01 14:40:49,083 - iris.src.initial_setup.ssl.ssl - INFO - SSL certificate setup skipped in local environment\n",
      "2025-04-01 14:40:49,083 - iris.src.initial_setup.oauth.oauth - INFO - Using API key authentication from local settings\n",
      "2025-04-01 14:40:49,083 - iris.src.initial_setup.oauth.oauth - INFO - Using OpenAI API key from settings: sk-proj...\n",
      "2025-04-01 14:40:49,083 - root - INFO - Processing input conversation...\n",
      "2025-04-01 14:40:49,083 - iris.src.conversation_setup.conversation - INFO - Processed conversation: 4 messages filtered to 3 messages\n",
      "2025-04-01 14:40:49,084 - root - INFO - Conversation processed: 3 messages\n",
      "2025-04-01 14:40:49,084 - root - INFO - Getting routing decision...\n",
      "2025-04-01 14:40:49,084 - iris.src.agents.agent_router.router - INFO - Getting routing decision using model: gpt-4o-mini-2024-07-18\n",
      "2025-04-01 14:40:49,100 - iris.src.llm_connectors.rbc_openai - INFO - Using API key: sk-proj...\n",
      "2025-04-01 14:40:49,101 - iris.src.llm_connectors.rbc_openai - INFO - Making non-streaming call to model: gpt-4o-mini-2024-07-18 with tools in local environment\n",
      "2025-04-01 14:40:49,101 - iris.src.llm_connectors.rbc_openai - INFO - Attempt 1/3: Sending request to OpenAI API\n",
      "2025-04-01 14:40:52,722 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-01 14:40:52,730 - iris.src.llm_connectors.rbc_openai - INFO - Received response in 3.63 seconds\n",
      "2025-04-01 14:40:52,730 - iris.src.llm_connectors.rbc_openai - INFO - Token usage - Completion: 9 ($0.0000), Prompt: 1208 ($0.0000), Total: 1217 tokens, Total Cost: $0.0000\n",
      "2025-04-01 14:40:52,731 - iris.src.agents.agent_router.router - INFO - Routing decision: research_from_database\n",
      "2025-04-01 14:40:52,731 - root - INFO - Routing decision received: research_from_database\n",
      "2025-04-01 14:40:52,731 - root - INFO - Using research path based on routing decision\n",
      "2025-04-01 14:40:52,732 - root - INFO - Clarifying research needs...\n",
      "2025-04-01 14:40:52,732 - iris.src.agents.agent_clarifier.clarifier - INFO - Clarifying research needs using model: gpt-4o-mini-2024-07-18\n",
      "2025-04-01 14:40:52,743 - iris.src.llm_connectors.rbc_openai - INFO - Using API key: sk-proj...\n",
      "2025-04-01 14:40:52,744 - iris.src.llm_connectors.rbc_openai - INFO - Making non-streaming call to model: gpt-4o-mini-2024-07-18 with tools in local environment\n",
      "2025-04-01 14:40:52,744 - iris.src.llm_connectors.rbc_openai - INFO - Attempt 1/3: Sending request to OpenAI API\n",
      "2025-04-01 14:40:54,561 - httpx - INFO - HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
      "2025-04-01 14:40:54,564 - iris.src.llm_connectors.rbc_openai - INFO - Received response in 1.82 seconds\n",
      "2025-04-01 14:40:54,565 - iris.src.llm_connectors.rbc_openai - INFO - Token usage - Completion: 77 ($0.0000), Prompt: 1403 ($0.0000), Total: 1480 tokens, Total Cost: $0.0000\n",
      "2025-04-01 14:40:54,566 - iris.src.agents.agent_clarifier.clarifier - INFO - Clarifier decision: request_essential_context\n",
      "2025-04-01 14:40:54,566 - iris.src.agents.agent_clarifier.clarifier - INFO - Is continuation: False\n",
      "2025-04-01 14:40:54,567 - root - INFO - Essential context needed, returning context questions\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response type: context_questions\n",
      "1. Which accounting standard are you following (e.g., IFRS, US GAAP)?\n",
      "2. What is the time period or fiscal year of interest for this revenue recognition?\n",
      "3. Are there any specific industries or types of long-term contracts you are focusing on?\n"
     ]
    }
   ],
   "source": [
    "# Call the model function with our conversation\n",
    "result_generator = model(conversation)\n",
    "\n",
    "# Access the initial chunk with metadata\n",
    "initial_chunk = next(result_generator)\n",
    "print(f\"Response type: {initial_chunk.get('type', 'None')}\")\n",
    "\n",
    "# Stream the response chunks\n",
    "for chunk in result_generator:\n",
    "    if chunk.get('response_chunk'):\n",
    "        print(chunk['response_chunk'], end=\"\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8191d-2940-454c-8f77-2768a015a9d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
